{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seg_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'modeling'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5ec5a8901f30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodeling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSRNsModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'modeling'"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "import configargparse\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from modeling import SRNsModel\n",
    "import util\n",
    "\n",
    "_LOG_ROOT = '/mnt/data/new_disk/liury/log/SRN/050816human_seg/'\n",
    "\n",
    "_MODEL_PATH = os.path.join(_LOG_ROOT, 'checkpoints/epoch_0094_iter_050000.pth')\n",
    "\n",
    "_TOT_NUM_INSTANCES = 184\n",
    "\n",
    "model = SRNsModel(num_instances=_TOT_NUM_INSTANCES,\n",
    "                  latent_dim=256,\n",
    "                  use_unet_renderer=False,\n",
    "                  tracing_steps=10,\n",
    "                  freeze_networks=True,\n",
    "                  out_channels=12,\n",
    "                  img_sidelength=128,\n",
    "                  output_sidelength=512\n",
    "                 )\n",
    "\n",
    "util.custom_load(model, path=_MODEL_PATH, discriminator=None,\n",
    "                 overwrite_embeddings=False, overwrite_cam=True)\n",
    "\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataset.face_dataset import FaceRandomPoseDataset\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "\n",
    "_OUTPUT_DIR = os.path.join(_LOG_ROOT, 'output_iter_020000')\n",
    "_MODE = 'sphere'\n",
    "_R = 1.5\n",
    "_CAM_INT = '/data/anpei/facial-data/seg_human/intrinsics.txt'\n",
    "\n",
    "_CAM_CENTER = [0.0, 1.6, 0.0]\n",
    "\n",
    "_NUM_INSTANCES=10\n",
    "_NUM_OBSERVATIONS=25\n",
    "\n",
    "output_dir = os.path.join(_OUTPUT_DIR, _MODE+'_128')\n",
    "sample_instances = list(np.random.choice(range(_TOT_NUM_INSTANCES), _NUM_INSTANCES, replace=False))\n",
    "\n",
    "# writer = SummaryWriter(output_dir)\n",
    "\n",
    "dataset = FaceRandomPoseDataset(\n",
    "    intrinsics=_CAM_INT,\n",
    "    cam_center=_CAM_CENTER,\n",
    "    num_instances=sample_instances, \n",
    "    num_observations=_NUM_OBSERVATIONS, \n",
    "    sample_radius=_R, mode=_MODE)\n",
    "\n",
    "dataloader = DataLoader(dataset,\n",
    "                     collate_fn=dataset.collate_fn,\n",
    "                     batch_size=1,\n",
    "                     shuffle=False,\n",
    "                     drop_last=False)\n",
    "\n",
    "print('Beginning evaluation...')\n",
    "\n",
    "images = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for idx, model_input in enumerate(dataloader):\n",
    "        model_input, ground_truth = model_input\n",
    "        \n",
    "        pose = model_input['pose']\n",
    "        \n",
    "#         print(pose)\n",
    "        \n",
    "        intrinsics = model_input['intrinsics']\n",
    "        uv = model_input['uv']\n",
    "        z = model.get_embedding(model_input)\n",
    "        \n",
    "        model_outputs = model(pose, z, intrinsics, uv)\n",
    "        predictions, depth_maps = model_outputs\n",
    "                \n",
    "        batch_size, tensor_len, channels = predictions.shape\n",
    "        img_sidelen = np.sqrt(tensor_len).astype(int)\n",
    "                \n",
    "        pred = torch.argmax(predictions, dim=2, keepdim=True)\n",
    "        output_img = util.lin2img(pred, color_map=dataset.color_map).cpu().numpy()\n",
    "        output_pred = pred.view(batch_size, img_sidelen, img_sidelen, 1).cpu().numpy()\n",
    "#         output_dpt = util.lin2img(depth_maps).cpu().numpy()\n",
    "#         output_dpt = (output_dpt - np.min(output_dpt)) / (np.max(output_dpt)-np.min(output_dpt) + 1e-5)\n",
    "#         print(idx, 'depth = ', np.max(output_dpt), np.min(output_dpt), output_dpt.shape)\n",
    "        \n",
    "#         model.write_updates(writer, model_outputs, iter=idx, mode='test')\n",
    "\n",
    "        for i in range(output_img.shape[0]):\n",
    "            instance_idx = int(model_input['instance_idx'][i].squeeze().detach().cpu().numpy().astype(np.int64))\n",
    "            observation_idx = model_input['observation_idx'][i]\n",
    "            \n",
    "            instance_dir = os.path.join(output_dir, \"%03d\" % instance_idx)\n",
    "            os.makedirs(instance_dir, exist_ok=True)\n",
    "            \n",
    "            img = output_img[i, :, :, :].squeeze().transpose(1, 2, 0)\n",
    "            img *= 255\n",
    "            img = img.round().clip(0, 255).astype(np.uint8)\n",
    "#             img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "\n",
    "            if not instance_idx in images:\n",
    "                images[instance_idx] = [None] * _NUM_OBSERVATIONS\n",
    "\n",
    "            images[instance_idx][observation_idx] = img\n",
    "#             output_fp = os.path.join(instance_dir, '%02d.png'%(observation_idx))\n",
    "#             util.write_img(img, output_fp)\n",
    "            \n",
    "            output_fp = os.path.join(instance_dir, '%02d_seg.png'%(observation_idx))\n",
    "            seg_img = output_pred[i, :, :].squeeze().astype(np.uint8)\n",
    "            \n",
    "            util.write_img(seg_img, output_fp)\n",
    "            \n",
    "#             dpt_img = (output_dpt[i, :, :, :].squeeze() * 255.0).astype(np.uint8)\n",
    "#             dpt_img = cv2.applyColorMap(dpt_img, cv2.COLORMAP_JET)\n",
    "#             output_fp = os.path.join(instance_dir, '%02d_depth.png'%(observation_idx))\n",
    "# #             print('Save output for instance %03d - %02d: %s'%(instance_idx, observation_idx, output_fp))\n",
    "#             util.write_img(dpt_img, output_fp)\n",
    "            print('Save output for instance %04d - %02d: %s'%(instance_idx, observation_idx, output_fp))\n",
    "        \n",
    "            if observation_idx == _NUM_OBSERVATIONS - 1:\n",
    "                imageio.mimsave(os.path.join(instance_dir, 'output.gif'), images[instance_idx], fps=5.0)\n",
    "                print('=== [DONE] saving output.gif.')\n",
    "            \n",
    "#         print('[DONE] Save output for instance %03d.'%(instance_idx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seg face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT embedding] variable.\n",
      "[INIT renderer] FC, with renderer = FC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SRNsModel(\n",
       "  (latent_codes): Embedding(664, 256)\n",
       "  (hyper_phi): HyperFC(\n",
       "    (layers): ModuleList(\n",
       "      (0): NewCls(\n",
       "        (hyper_linear): HyperLinear(\n",
       "          (hypo_params): FCBlock(\n",
       "            (net): Sequential(\n",
       "              (0): FCLayer(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                  (2): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): FCLayer(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                  (2): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (2): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm_nl): Sequential(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): NewCls(\n",
       "        (hyper_linear): HyperLinear(\n",
       "          (hypo_params): FCBlock(\n",
       "            (net): Sequential(\n",
       "              (0): FCLayer(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                  (2): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): FCLayer(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                  (2): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (2): Linear(in_features=256, out_features=65792, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm_nl): Sequential(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (2): NewCls(\n",
       "        (hyper_linear): HyperLinear(\n",
       "          (hypo_params): FCBlock(\n",
       "            (net): Sequential(\n",
       "              (0): FCLayer(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                  (2): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): FCLayer(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                  (2): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (2): Linear(in_features=256, out_features=65792, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm_nl): Sequential(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (3): NewCls(\n",
       "        (hyper_linear): HyperLinear(\n",
       "          (hypo_params): FCBlock(\n",
       "            (net): Sequential(\n",
       "              (0): FCLayer(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                  (2): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (1): FCLayer(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                  (2): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (2): Linear(in_features=256, out_features=65792, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm_nl): Sequential(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ray_marcher): Raymarcher(\n",
       "    (lstm): LSTMCell(256, 16)\n",
       "    (out_layer): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       "  (pixel_generator): FCBlock(\n",
       "    (net): Sequential(\n",
       "      (0): FCLayer(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): FCLayer(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (2): FCLayer(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (3): FCLayer(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (4): FCLayer(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Linear(in_features=256, out_features=20, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (l2_loss): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys,os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from modeling import SRNsModel\n",
    "import util\n",
    "\n",
    "_RENDERER = 'FC'\n",
    "\n",
    "# _MODEL_PATH = '../checkpoints/epoch_0020_iter_090000.pth'\n",
    "# _TOT_NUM_INSTANCES = 1494\n",
    "# _OPT_CAM = False\n",
    "\n",
    "# _MODEL_PATH = '../checkpoints/050701face_seg_2000_depth.pth'\n",
    "# _OPT_CAM = False\n",
    "# _TOT_NUM_INSTANCES = 3714\n",
    "\n",
    "# _MODEL_PATH = '/root/liury/log/SRNs/060706face_celebA/checkpoints/epoch_1359_iter_170000.pth'\n",
    "# _OPT_CAM = True\n",
    "# _TOT_NUM_INSTANCES = 1000\n",
    "\n",
    "\n",
    "# _MODEL_PATH = '/root/liury/log/SRNs/061905seg_teaser/checkpoints/epoch_2400_iter_012000.pth'\n",
    "# _OPT_CAM = True\n",
    "# _TOT_NUM_INSTANCES = 37\n",
    "\n",
    "\n",
    "# _MODEL_PATH = '../checkpoints/061721face_seg_800.pth'\n",
    "# _OPT_CAM = False\n",
    "# _TOT_NUM_INSTANCES = 791\n",
    "\n",
    "# _MODEL_PATH = '/home/anpei/liury/log/SRNs/062923face_seg_800/checkpoints/epoch_0036_iter_110000.pth'\n",
    "# _OPT_CAM = False\n",
    "# _TOT_NUM_INSTANCES = 711\n",
    "\n",
    "_MODEL_PATH = '/home/anpei/liury/log/SRNs/070212face_seg_800/checkpoints/epoch_0010_iter_030000.pth'\n",
    "_OPT_CAM = False\n",
    "_TOT_NUM_INSTANCES = 664\n",
    "\n",
    "\n",
    "# _MODEL_PATH = '/home/anpei/liury/log/SRNs/070821face_seg_800_imae/checkpoints/epoch_0007_iter_020000.pth'\n",
    "# _OPT_CAM = False\n",
    "# _TOT_NUM_INSTANCES = 664\n",
    "# _RENDERER = 'ImAE'\n",
    "\n",
    "# _MODEL_PATH = '/home/anpei/liury/log/SRNs/061701face_celebA/checkpoints/epoch_0840_iter_420000.pth'\n",
    "# _OPT_CAM = True\n",
    "# _TOT_NUM_INSTANCES = 4000\n",
    "\n",
    "# _MODEL_PATH = os.path.join(\n",
    "#     os.getenv(\"HOME\"), 'liury/log/SRNs/061916face_celebA/checkpoints/epoch_2000_iter_014000.pth')\n",
    "# _LOG_ROOT = os.path.join(\n",
    "#     os.getenv(\"HOME\"), 'liury/log/SRNs/061916face_celebA/reproj_celebA')\n",
    "# os.makedirs(os.path.join(_LOG_ROOT, 'vis'), exist_ok=True)\n",
    "# _OPT_CAM=True\n",
    "\n",
    "# _TOT_NUM_INSTANCES = 50\n",
    "# _TOT_NUM_INSTANCES = 3714\n",
    "\n",
    "_IMG_SIZE = 128\n",
    "_OUT_SIZE = 512\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "model = SRNsModel(num_instances=_TOT_NUM_INSTANCES,\n",
    "                  latent_dim=256,\n",
    "                  renderer=_RENDERER,\n",
    "                  tracing_steps=10,\n",
    "                  freeze_networks=True,\n",
    "                  out_channels=20,\n",
    "                  img_sidelength=_IMG_SIZE,\n",
    "                  output_sidelength=_OUT_SIZE,\n",
    "                  opt_cam=_OPT_CAM\n",
    "                 )\n",
    "\n",
    "util.custom_load(model, path=_MODEL_PATH, discriminator=None,\n",
    "                 overwrite_embeddings=False, overwrite_cam=True)\n",
    "\n",
    "model.eval()\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.face_dataset import _campos2matrix\n",
    "import cv2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "_CMAP = np.asarray([[0, 0, 0], [127, 212, 255], [255, 255, 127], [255, 255, 127], # 'background','skin', 'l_brow', 'r_brow'\n",
    "                    [255, 255, 170], [255, 255, 170], [240, 157, 240], [255, 212, 255], #'l_eye', 'r_eye', 'r_nose', 'l_nose',\n",
    "                    [31, 162, 230], [127, 255, 255], [127, 255, 255], #'mouth', 'u_lip', 'l_lip'\n",
    "                    [0, 255, 85], [0, 255, 85], [0, 255, 170], [255, 255, 170], #'l_ear', 'r_ear', 'ear_r', 'eye_g'\n",
    "                    [127, 170, 255], [85, 0, 255], [255, 170, 127], #'neck', 'neck_l', 'cloth'\n",
    "                    [212, 127, 255], [0, 170, 255]#, 'hair', 'hat'\n",
    "                    ])\n",
    "\n",
    "_CMAP =torch.tensor(_CMAP, dtype=torch.float32) / 255.0\n",
    "\n",
    "def _build_cam_int(focal, H, W):\n",
    "    return np.array([  [focal, 0., W // 2, 0.],\n",
    "                       [0., focal, H // 2, 0],\n",
    "                       [0., 0, 1, 0],\n",
    "                       [0, 0, 0, 1]])\n",
    "\n",
    "\n",
    "def render_scene(model, pose, z, focal, img_sidelength):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pose = torch.from_numpy(pose).float().unsqueeze(0)\n",
    "        cam_int = torch.from_numpy(\n",
    "            _build_cam_int(focal, _IMG_SIZE, _IMG_SIZE)).float().unsqueeze(0)\n",
    "\n",
    "        uv = np.mgrid[0:_IMG_SIZE, 0:_IMG_SIZE].astype(np.int32)\n",
    "        uv = torch.from_numpy(np.flip(uv, axis=0).copy()).long()\n",
    "        uv = uv.reshape(2, -1).transpose(1, 0).unsqueeze(0)\n",
    "\n",
    "#         print(pose.shape, cam_int.shape, uv.shape, z.shape)\n",
    "\n",
    "        predictions, depth_maps = model(pose, z, cam_int, uv)\n",
    "\n",
    "        pred = torch.argmax(predictions, dim=2, keepdim=True)\n",
    "\n",
    "#         print(pred.shape)\n",
    "\n",
    "        out_img = util.lin2img(pred, color_map=_CMAP).cpu().numpy()\n",
    "        out_seg = pred.view(img_sidelength, img_sidelength, 1).cpu().numpy()\n",
    "        \n",
    "        out_img = (out_img.squeeze().transpose(1, 2, 0)) * 255.0\n",
    "        out_img = out_img.round().clip(0, 255).astype(np.uint8)\n",
    "\n",
    "#         output_fp = os.path.join(instance_dir, '%02d_seg.png'%(observation_idx))\n",
    "        out_seg = out_seg.squeeze().astype(np.uint8)\n",
    "\n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         plt.imshow(out_img)\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         plt.imshow(out_seg)\n",
    "\n",
    "#         plt.show()\n",
    "\n",
    "        return out_img, out_seg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import imageio\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from dataset.face_dataset import _campos2matrix\n",
    "from dataset import data_util\n",
    "\n",
    "def render_custo_path(rener_mode, src_latent, init_dpt=0.8, trgt_latent=None, cam_center=np.array([0,0.11,0.0]), output_dir='output_custom', offset=0):\n",
    "    \n",
    "    _DEFAULT_CAM_INT = '../checkpoints/intrinsics.txt'\n",
    "    cam_int = data_util.parse_intrinsics(_DEFAULT_CAM_INT, trgt_sidelength=128)\n",
    "    focal = cam_int[0, 0]\n",
    "\n",
    "    img_outputs = []\n",
    "    seg_outputs = []\n",
    "\n",
    "    idx = offset\n",
    "    steps = 20\n",
    "        \n",
    "    def _get_latent(idx, tot_steps=steps, inv=False):\n",
    "        if trgt_latent is None:\n",
    "            return src_latent\n",
    "        else:\n",
    "            _w = idx / (steps-1)\n",
    "            if inv:\n",
    "                _w = (1.0 - _w)\n",
    "            \n",
    "            return (1.0-_w)*src_latent + _w*trgt_latent\n",
    "      \n",
    "    dpt = init_dpt\n",
    "    cam_pose = _campos2matrix(np.array([0., 0., dpt])+cam_center, cam_center)\n",
    "    \n",
    "##################### fly in\n",
    "#     print('*********************** FLY IN ***********************', idx)    \n",
    "\n",
    "#     _range = np.asarray([-1.0, 1.0])*0.2 + init_dpt\n",
    "#     dpts = np.concatenate([\n",
    "#         np.linspace(init_dpt, _range[0], 5), \n",
    "#         np.linspace(_range[0], _range[1], 10),\n",
    "#         np.linspace(_range[1], init_dpt, 5)], axis=0)\n",
    "\n",
    "#     _range = 0.2\n",
    "#     dpts = np.linspace(init_dpt, init_dpt-_range, steps)\n",
    "    \n",
    "#     for i, dpt in enumerate(dpts):\n",
    "#         cam_pose = _campos2matrix(\n",
    "#             np.array([0., 0., dpt])+cam_center,\n",
    "#             cam_center)\n",
    "\n",
    "#         out_img, out_seg = render_scene(model, cam_pose, _get_latent(i), focal, _OUT_SIZE)\n",
    "#         img_outputs.append(out_img)\n",
    "#         output_fp = os.path.join(output_dir, '%04d.png'%(idx))\n",
    "#         util.write_img(out_seg, output_fp)\n",
    "\n",
    "#         idx += 1\n",
    "\n",
    "##################### zoom in\n",
    "    \n",
    "    focal_scale = 1.25\n",
    "    focals = np.linspace(focal, focal*focal_scale, steps)\n",
    "    for i, focal in enumerate(focals):\n",
    "        out_img, out_seg = render_scene(model, cam_pose, _get_latent(i), focal, _OUT_SIZE)\n",
    "        img_outputs.append(out_img)\n",
    "        output_fp = os.path.join(output_dir, '%04d.png'%(idx))\n",
    "        util.write_img(out_seg, output_fp)\n",
    "\n",
    "        idx += 1\n",
    "        \n",
    "\n",
    "# ##################### moving x\n",
    "#     _range = np.asarray([-1.0, 1.0])*0.05 / focal_scale\n",
    "#     _x = np.concatenate([\n",
    "#         np.linspace(0, _range[0], int(steps/2)), \n",
    "#         np.linspace(_range[0], _range[1], steps),\n",
    "#         np.linspace(_range[1], 0, int(steps/2))], axis=0)\n",
    "\n",
    "#     for i, x in enumerate(_x):\n",
    "#         if rener_mode == 0:\n",
    "#             cam_pose = _campos2matrix(\n",
    "#                 np.array([x, 0, dpt]+cam_center), \n",
    "#                 np.array([x, 0, dpt-1]+cam_center))\n",
    "\n",
    "#         elif rener_mode == 1:    \n",
    "#             cam_pose = _campos2matrix(\n",
    "#                 np.array([x*6, 0, dpt]+cam_center), \n",
    "#                 cam_center)\n",
    "            \n",
    "#         out_img, out_seg = render_scene(model, cam_pose, _get_latent(i, 20), focal, _OUT_SIZE)\n",
    "#         img_outputs.append(out_img)\n",
    "\n",
    "#         output_fp = os.path.join(output_dir, '%04d.png'%(idx))\n",
    "#         util.write_img(out_seg, output_fp)\n",
    "\n",
    "#         idx += 1\n",
    "\n",
    "\n",
    "# ##################### moving y\n",
    "#     _range = np.asarray([-0.8, 1.2])*0.05 / focal_scale\n",
    "#     _y = np.concatenate([\n",
    "#         np.linspace(0, _range[0], int(steps/2)), \n",
    "#         np.linspace(_range[0], _range[1], steps),\n",
    "#         np.linspace(_range[1], 0, int(steps/2))], axis=0)\n",
    "\n",
    "#     for i, y in enumerate(_y):\n",
    "#         if rener_mode == 0:\n",
    "#             cam_pose = _campos2matrix(\n",
    "#                 np.array([0, y, dpt]+cam_center), \n",
    "#                 np.array([0, y, dpt-1]+cam_center), )\n",
    "\n",
    "#         elif rener_mode == 1:\n",
    "#             cam_pose = _campos2matrix(\n",
    "#                 np.array([0, y*6, dpt]+cam_center), \n",
    "#                 cam_center.reshape((1, 3)))\n",
    "\n",
    "#         out_img, out_seg = render_scene(model, cam_pose, _get_latent(i, 20), focal, _OUT_SIZE)\n",
    "#         img_outputs.append(out_img)\n",
    "\n",
    "#         output_fp = os.path.join(output_dir, '%04d.png'%(idx))\n",
    "#         util.write_img(out_seg, output_fp)\n",
    "\n",
    "#         idx += 1\n",
    "                    \n",
    "            \n",
    "        ##################### Zoom Out \n",
    "#     _range = np.asarray([-1.0, 1.0])*0.3*focal + focal\n",
    "#     focals = np.concatenate([\n",
    "#         np.linspace(focal, _range[0], 5),\n",
    "#         np.linspace(_range[0], _range[1], 20),\n",
    "#         np.linspace(_range[1], focal, 5)], axis=0)\n",
    "\n",
    "    focals = np.linspace(focal, focal/focal_scale, int(steps/2))\n",
    "    for i, focal in enumerate(focals):\n",
    "        out_img, out_seg = render_scene(model, cam_pose, _get_latent(i), focal, _OUT_SIZE)\n",
    "        img_outputs.append(out_img)\n",
    "        output_fp = os.path.join(output_dir, '%04d.png'%(idx))\n",
    "        util.write_img(out_seg, output_fp)\n",
    "\n",
    "        idx += 1\n",
    "        \n",
    "    ##################### Huygens    \n",
    "    R = dpt\n",
    "    theta = np.linspace(0, np.pi*2, num=steps*2)\n",
    "    x = R * 0.4 * np.sin(theta)\n",
    "    y = R * 0.2 * np.sin(theta) * np.cos(theta)\n",
    "    \n",
    "    for i in range(len(theta)):\n",
    "        dpt = np.sqrt(R**2-x[i]**2-y[i]**2)\n",
    "        cam_pose = _campos2matrix(\n",
    "            np.array([x[i], y[i], dpt]+cam_center),\n",
    "            cam_center)\n",
    "                \n",
    "        out_img, out_seg = render_scene(model, cam_pose, _get_latent(i), focal, _OUT_SIZE)\n",
    "        \n",
    "        img_outputs.append(out_img)\n",
    "        \n",
    "        output_fp = os.path.join(output_dir, '%04d.png'%(idx))\n",
    "        util.write_img(out_seg, output_fp)\n",
    "        \n",
    "        idx += 1\n",
    "    dpt = R\n",
    "        \n",
    "        \n",
    "#     # spherical\n",
    "#     steps = 30\n",
    "    \n",
    "#     theta = np.linspace(np.pi/2-0.2, np.pi/2+0.2, num=steps)\n",
    "#     phi = np.linspace(np.pi/2-0.6, np.pi/2+0.6, num=steps)\n",
    "    \n",
    "#     x = R * np.sin(theta) * np.cos(phi)\n",
    "#     y = R * np.sin(theta) * np.sin(phi)\n",
    "#     z = R * np.cos(theta)\n",
    "    \n",
    "#     for i in range(steps):\n",
    "#         cam_pose = _campos2matrix(\n",
    "#             np.array([x[i], z[i], y[i]]+cam_center), \n",
    "#             cam_center.reshape((1, 3)))\n",
    "                \n",
    "#         out_img, out_seg = render_scene(model, cam_pose, _get_latent(i), focal, _OUT_SIZE)\n",
    "        \n",
    "#         img_outputs.append(out_img)\n",
    "        \n",
    "#         output_fp = os.path.join(output_dir, '%04d.png'%(idx))\n",
    "#         util.write_img(out_seg, output_fp)\n",
    "        \n",
    "#         idx += 1\n",
    "        \n",
    "#     # spherical reverse\n",
    "#     R = 1.1\n",
    "#     steps = 30\n",
    "    \n",
    "#     theta = np.linspace(np.pi/2-0.2, np.pi/2+0.2, num=steps)\n",
    "#     phi = np.linspace(np.pi/2+0.8, np.pi/2-0.8, num=steps)\n",
    "    \n",
    "#     x = R * np.sin(theta) * np.cos(phi)\n",
    "#     y = R * np.sin(theta) * np.sin(phi)\n",
    "#     z = R * np.cos(theta)\n",
    "    \n",
    "#     for i in range(steps):\n",
    "#         cam_pose = _campos2matrix(\n",
    "#             np.array([x[i], z[i], y[i]]+cam_center), \n",
    "#             cam_center.reshape((1, 3)))\n",
    "        \n",
    "#         out_img, out_seg = render_scene(model, cam_pose, _get_latent(i, inv=True), focal, _OUT_SIZE)\n",
    "        \n",
    "#         img_outputs.append(out_img)\n",
    "        \n",
    "#         output_fp = os.path.join(output_dir, '%04d.png'%(idx))\n",
    "#         util.write_img(out_seg, output_fp)\n",
    "        \n",
    "#         idx += 1\n",
    "\n",
    "    \n",
    "#     print('output_dir = ', output_dir)    \n",
    "#     imageio.mimsave(os.path.join(output_dir, 'output.gif'), img_outputs, fps=15.0)\n",
    "        \n",
    "#     # spiral\n",
    "# #     print('*********************** SPIRAL ***********************', idx)\n",
    "\n",
    "#     R = 0.2\n",
    "#     theta = np.linspace(0, np.pi*7, num=31)\n",
    "#     r = np.log(np.linspace(0, 1, num=31))*R\n",
    "#     x = -r * np.sin(theta)\n",
    "#     y = r * np.cos(theta)\n",
    "\n",
    "#     plt.plot(x, y)\n",
    "#     plt.show()\n",
    "\n",
    "#     for cur_t in range(x.shape[0]-1):\n",
    "#         cam_pose = _campos2matrix(\n",
    "#             np.array([x[cur_t], y[cur_t], dpt]+cam_center), \n",
    "#             cam_center.reshape((1, 3)))\n",
    "#         out_img, out_seg = render_scene(model, cam_pose, latent, focal, _OUT_SIZE)\n",
    "\n",
    "#         img_outputs.append(out_img)\n",
    "\n",
    "#     #     output_fp = os.path.join(output_dir, '%02d.png'%(idx))\n",
    "#     #     util.write_img(out_img, output_fp)\n",
    "#         output_fp = os.path.join(output_dir, '%04d.png'%(idx))\n",
    "#         util.write_img(out_seg, output_fp)\n",
    "\n",
    "#         idx += 1\n",
    "\n",
    "#     output_fp = os.path.join(output_dir, '..', 'vis', os.path.basename(output_dir)+'.gif')\n",
    "#     print('[DONE] save vis to : ', output_fp)\n",
    "#     imageio.mimsave(output_fp, img_outputs, fps=10.0)\n",
    "    \n",
    "    return img_outputs, cam_pose, focal, dpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## 17 cls ##########################################\n",
    "### ['background'0,'skin'1, 'l_brow'2, 'r_brow'2, 'l_eye'3, 'r_eye'3,'r_nose'4, 'l_nose'5, 'mouth'6, 'u_lip'7,\n",
    "### 'l_lip'8, 'l_ear'9, 'r_ear'9, 'ear_r'10, 'eye_g'11, 'neck'12, 'neck_l'13, 'cloth'14, 'hair'15, 'hat'16]\n",
    "\n",
    "# _REQUIRED_PARTS = set([0, 1,3,4,5,7,8])\n",
    "# _SKIPED_PARTS = set([11])\n",
    "# _NOSE_IDX = [5, 4]\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "########################################## 20 cls ##########################################\n",
    "####  ['background'0,'skin'1, 'l_brow'2, 'r_brow'3, 'l_eye'4, 'r_eye'5,'r_nose'6, 'l_nose'7, 'mouth'8, 'u_lip'9,\n",
    "#### 'l_lip'10, 'l_ear'11, 'r_ear'12, 'ear_r'13, 'eye_g'14, 'neck'15, 'neck_l'16, 'cloth'17, 'hair'18, 'hat'19]\n",
    "\n",
    "_REQUIRED_PARTS = set([0, 1, 4, 5, 6, 9, 10])\n",
    "_SKIPED_PARTS = set([14])\n",
    "_NOSE_IDX = [6, 7]\n",
    "_DEFAULT_CAM_INT = '../checkpoints/intrinsics.txt'\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "_CAM_K = '../checkpoints/intrinsics.txt'\n",
    "_CAM_T = cam_center = np.asarray([0., 0.11, 1.3])\n",
    "_CAM_CENTER = cam_center = np.asarray([0., 0.11, 0.1])\n",
    "\n",
    "\n",
    "def render_sample(latent,\n",
    "                  output_dir,\n",
    "                  offset=0,\n",
    "                  init_dpt=1.0,\n",
    "                  cam_K=_CAM_K, \n",
    "                  cam_center=_CAM_CENTER,\n",
    "                  validate=True):\n",
    "    \n",
    "    cam_int = data_util.parse_intrinsics(cam_K, trgt_sidelength=128)\n",
    "    focal = cam_int[0, 0]\n",
    "    cx, cy = cam_int[:2, 2]\n",
    "\n",
    "    if _OPT_CAM:\n",
    "        cam_T = (latent[:, :3]).clone().detach().squeeze().cpu().numpy()\n",
    "        init_dpt = np.linalg.norm(cam_T-cam_center)\n",
    "    else:\n",
    "        cam_T = np.asarray([0.0, 0.0, init_dpt]) + cam_center\n",
    "        \n",
    "    cam_pose = _campos2matrix(cam_T, cam_center)\n",
    "    \n",
    "    if validate:\n",
    "        out_img, out_seg = render_scene(model, cam_pose, latent, focal, _OUT_SIZE)\n",
    "        all_parts = set(np.unique(out_seg)) \n",
    "\n",
    "        while (_SKIPED_PARTS.issubset(all_parts) or not _REQUIRED_PARTS.issubset(all_parts)):\n",
    "            print('Resample latent code: ', all_parts)\n",
    "            lat_idx = torch.randint(0, _TOT_NUM_INSTANCES, (1,)).squeeze().cuda()\n",
    "            latent = model.get_embedding({'instance_idx': lat_idx}).unsqueeze(0)\n",
    "            out_img, out_seg = render_scene(model, cam_pose, latent, focal, _OUT_SIZE)\n",
    "            ##### skip images with eyeglasses\n",
    "            all_parts = set(np.unique(out_seg))\n",
    "\n",
    "        #### calculate campose from seg map\n",
    "        l_nose_idx = np.asarray(np.where(out_seg==_NOSE_IDX[0]))\n",
    "        r_nose_idx = np.asarray(np.where(out_seg==_NOSE_IDX[1]))\n",
    "        nose_idx = np.concatenate([l_nose_idx, r_nose_idx], axis=1)        \n",
    "        center = (np.mean(nose_idx[0]), np.mean(nose_idx[1]))\n",
    "\n",
    "        yy = (256 - center[0])/(focal*4)\n",
    "        xx = (center[1]-256.0)/(focal*4)\n",
    "\n",
    "        cam_center = cam_center + np.array([xx, yy, 0.0])\n",
    "        cam_T = np.array([0., 0.0, init_dpt]) + cam_center\n",
    "        \n",
    "        cam_pose = _campos2matrix(cam_T, cam_center)\n",
    "\n",
    "        ############## vis ##################    \n",
    "        out_img1, out_seg1 = render_scene(model, cam_pose, latent, focal, _OUT_SIZE)\n",
    "\n",
    "        l_nose_idx = np.asarray(np.where(out_seg1==5))\n",
    "        r_nose_idx = np.asarray(np.where(out_seg1==4))\n",
    "        nose_idx = np.concatenate([l_nose_idx, r_nose_idx], axis=1)\n",
    "\n",
    "        center1 = (np.mean(nose_idx[0]), np.mean(nose_idx[1]))    \n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(out_img)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(out_img1)\n",
    "        plt.show()\n",
    "        #####################################\n",
    "        \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    vis_outputs, cam_pose, focal, init_dpt = render_custo_path(\n",
    "        0, latent, init_dpt=init_dpt, cam_center=cam_center, output_dir=output_dir, offset=offset)\n",
    "    \n",
    "    return vis_outputs, latent, cam_pose, focal, init_dpt, cam_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging root =  /home/anpei/liury/log/SRNs/test/custo_070821face_seg_800_imae\n",
      "[00] 0000 -> 0591: /home/anpei/liury/log/SRNs/test/custo_070821face_seg_800_imae/s0000_t0591\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d6c003b1b9fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# render src-fvv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     outputs, src_latent, cam_pose, focal, init_dpt, cam_center = render_sample(\n\u001b[0;32m---> 39\u001b[0;31m         src_latent, output_dir, offset=len(vis_outputs), validate=False)\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mvis_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1c94c48d8421>\u001b[0m in \u001b[0;36mrender_sample\u001b[0;34m(latent, output_dir, offset, init_dpt, cam_K, cam_center, validate)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     vis_outputs, cam_pose, focal, init_dpt = render_custo_path(\n\u001b[0;32m---> 92\u001b[0;31m         0, latent, init_dpt=init_dpt, cam_center=cam_center, output_dir=output_dir, offset=offset)\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvis_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcam_pose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfocal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_dpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcam_center\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1d6cb1b921c1>\u001b[0m in \u001b[0;36mrender_custo_path\u001b[0;34m(rener_mode, src_latent, init_dpt, trgt_latent, cam_center, output_dir, offset)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mimg_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0moutput_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%04d.png'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_seg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/new_disk2/liury/code/SRNs/util.py\u001b[0m in \u001b[0;36mwrite_img\u001b[0;34m(img, path)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrite_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0min_out_to_param_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_out_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# _IDX_FP = os.path.join(os.getenv(\"HOME\"), 'liury/log/SRNs/061900seg_teaser/indexing.txt')\n",
    "# with open(_IDX_FP, 'r') as f:\n",
    "#     output_dirs = f.readlines()\n",
    "#     output_dirs = [os.path.basename(x.strip()).split('.')[0] for x in output_dirs]\n",
    "# print(output_dirs)\n",
    "\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "\n",
    "_LOG_ROOT = os.path.join(os.getenv(\"HOME\"), 'liury/log/SRNs/test', 'custo_070821face_seg_800_imae')\n",
    "os.makedirs(os.path.join(_LOG_ROOT, 'vis'), exist_ok=True)\n",
    "print('Logging root = ', _LOG_ROOT)\n",
    "\n",
    "_INTERP_STEPS = 30\n",
    "\n",
    "# all_instances = list(range(_TOT_NUM_INSTANCES))\n",
    "all_instances = list(np.loadtxt('/mnt/new_disk2/liury/log/SRNs/goodList.txt'))\n",
    "for i in range(664):\n",
    "\n",
    "#     lat_idx = random.choices(all_instances, k=2)\n",
    "    lat_idx = [all_instances[i], random.choice(all_instances)]\n",
    "    \n",
    "    src_idx = lat_idx[0]\n",
    "    trgt_idx = lat_idx[1] if lat_idx[1]!=lat_idx[0] else (lat_idx[0]+13)%_TOT_NUM_INSTANCES\n",
    "    \n",
    "    src_latent = model.get_embedding({'instance_idx': torch.LongTensor([src_idx]).squeeze().cuda()}).unsqueeze(0)\n",
    "    trgt_latent = model.get_embedding({'instance_idx': torch.LongTensor([trgt_idx]).squeeze().cuda()}).unsqueeze(0)\n",
    "        \n",
    "    vis_outputs = []\n",
    "    \n",
    "    output_dir = os.path.join(\n",
    "        _LOG_ROOT, 's%04d_t%04d'%(src_idx, trgt_idx))\n",
    "    os.makedirs(os.path.join(output_dir, 'seg'), exist_ok=True)\n",
    "    \n",
    "    print('[%02d] %04d -> %04d: %s'%(i, src_idx, trgt_idx, output_dir))\n",
    "    \n",
    "    # render src-fvv \n",
    "    outputs, src_latent, cam_pose, focal, init_dpt, cam_center = render_sample(\n",
    "        src_latent, output_dir, offset=len(vis_outputs), validate=False)\n",
    "    vis_outputs.extend(outputs)\n",
    "    \n",
    "    # render interpolation\n",
    "    cdf_scale = 1.0/(1.0-norm.cdf(-_INTERP_STEPS//2,0,6)*2)\n",
    "    for idx in range(-_INTERP_STEPS//2,_INTERP_STEPS//2+1):\n",
    "        _w = norm.cdf(idx,0,6)*cdf_scale\n",
    "        latent = (1.0-_w)*src_latent + _w*trgt_latent\n",
    "\n",
    "        out_img, out_seg = render_scene(model, cam_pose, latent, focal, _OUT_SIZE)        \n",
    "        output_fp = os.path.join(output_dir, '%04d.png'%(len(vis_outputs)))\n",
    "        util.write_img(out_seg, output_fp)\n",
    "        vis_outputs.append(out_img)\n",
    "    \n",
    "    # render trgt-fvv\n",
    "    outputs, _, _, _, _, _ = render_sample(\n",
    "        trgt_latent, output_dir, init_dpt=init_dpt, cam_center=cam_center, \n",
    "        offset=len(vis_outputs), validate=False)\n",
    "    vis_outputs.extend(outputs)\n",
    "    \n",
    "    vis_fp = os.path.join(_LOG_ROOT, 'vis', os.path.basename(output_dir)+'.gif')\n",
    "    print('[DONE] save vis to : ', vis_fp)\n",
    "    imageio.mimsave(vis_fp, vis_outputs, fps=20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging root =  /home/anpei/liury/log/SRNs/test/custo_070821face_seg_800_imae\n",
      "[[ 1.   -0.    0.    0.  ]\n",
      " [-0.   -1.    0.    0.11]\n",
      " [ 0.   -0.   -1.    1.1 ]\n",
      " [ 0.    0.    0.    1.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Vis all instances\n",
    "\n",
    "import random,os\n",
    "\n",
    "_LOG_ROOT = os.path.join(os.getenv(\"HOME\"), 'liury/log/SRNs/test', 'custo_070821face_seg_800_imae')\n",
    "os.makedirs(os.path.join(_LOG_ROOT, 'vis'), exist_ok=True)\n",
    "print('Logging root = ', _LOG_ROOT)\n",
    "\n",
    "_INTERP_STEPS = 2\n",
    "all_instances = list(range(_TOT_NUM_INSTANCES))\n",
    "for i in range(1):\n",
    "\n",
    "    lat_idx = [all_instances[i], random.choice(all_instances)]\n",
    "    src_idx = all_instances[i]\n",
    "    \n",
    "    src_latent = model.get_embedding({'instance_idx': torch.LongTensor([src_idx]).squeeze().cuda()}).unsqueeze(0)\n",
    "    \n",
    "    cam_K = '../checkpoints/intrinsics.txt'\n",
    "    cam_int = data_util.parse_intrinsics(cam_K, trgt_sidelength=128)\n",
    "    focal = cam_int[0, 0]\n",
    "    cx, cy = cam_int[:2, 2]\n",
    "\n",
    "    cam_center = np.asarray([0., 0.11, 0.1])\n",
    "    cam_T = np.asarray([0.0, 0.0, 1.0]) + cam_center\n",
    "    cam_pose = _campos2matrix(cam_T, cam_center)\n",
    "    print(cam_pose)\n",
    "\n",
    "\n",
    "    latent = src_latent \n",
    "\n",
    "    out_img, out_seg = render_scene(model, cam_pose, latent, focal, _OUT_SIZE)        \n",
    "\n",
    "    vis_fp = os.path.join(_LOG_ROOT, 'vis','%04d.png'%src_idx)\n",
    "#     print('[DONE] save vis to : ', vis_fp)\n",
    "    util.write_img(out_img[...,::-1], vis_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging root =  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0000_t0013.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0001_t0014.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0002_t0015.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0003_t0016.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0004_t0017.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0005_t0018.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0006_t0019.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0007_t0020.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0008_t0021.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0009_t0022.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0010_t0023.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0011_t0024.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0012_t0025.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0013_t0026.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0014_t0027.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0015_t0028.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0016_t0029.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0017_t0030.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0018_t0031.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0019_t0032.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0020_t0033.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0021_t0034.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0022_t0035.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0023_t0036.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0024_t0037.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0025_t0038.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0026_t0039.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0027_t0040.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0028_t0041.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0029_t0042.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0030_t0043.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0031_t0044.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0032_t0045.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0033_t0046.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0034_t0047.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0035_t0048.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0036_t0049.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0037_t0050.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0038_t0051.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0039_t0052.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0040_t0053.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0041_t0054.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0042_t0055.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0043_t0056.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0044_t0057.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0045_t0058.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0046_t0059.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0047_t0060.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0048_t0061.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0049_t0062.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0050_t0063.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0051_t0064.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0052_t0065.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0053_t0066.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0054_t0067.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0055_t0068.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0056_t0069.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0057_t0070.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0058_t0071.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0059_t0072.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0060_t0073.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0061_t0074.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0062_t0075.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0063_t0076.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0064_t0077.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0065_t0078.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0066_t0079.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0067_t0080.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0068_t0081.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0069_t0082.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0070_t0083.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0071_t0084.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0072_t0085.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0073_t0086.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0074_t0087.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0075_t0088.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0076_t0089.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0077_t0090.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0078_t0091.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0079_t0092.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0080_t0093.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0081_t0094.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0082_t0095.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0083_t0096.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0084_t0097.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0085_t0098.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0086_t0099.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0087_t0100.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0088_t0101.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0089_t0102.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0090_t0103.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0091_t0104.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0092_t0105.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0093_t0106.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0094_t0107.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0095_t0108.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0096_t0109.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0097_t0110.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0098_t0111.gif\n",
      "[DONE] save vis to :  /home/anpei/liury/log/SRNs/test/spiral_comp/070212face_seg_800/vis/s0099_t0112.gif\n"
     ]
    }
   ],
   "source": [
    "# render with spiral path\n",
    "\n",
    "import random,os\n",
    "\n",
    "\n",
    "lookat = np.asarray([0., 0.12, 0.0])\n",
    "cam_center =  np.asarray([0., 0.12, 0.97])\n",
    "radii,focus_depth = [0.1,0.3,0.2],1.0 # z,x,y\n",
    "# cam_poses = create_spiral_poses(radii,focus_depth)\n",
    "\n",
    "# _LOG_ROOT = os.path.join(os.getenv(\"HOME\"), 'liury/log/SRNs/test', 'spiral_comp', '070821face_seg_800_imae')\n",
    "\n",
    "_LOG_ROOT = os.path.join(os.getenv(\"HOME\"), 'liury/log/SRNs/test', 'spiral_comp', '070212face_seg_800')\n",
    "os.makedirs(os.path.join(_LOG_ROOT, 'vis'), exist_ok=True)\n",
    "print('Logging root = ', _LOG_ROOT)\n",
    "\n",
    "_INTERP_STEPS = 30\n",
    "# all_instances = [422,207,278,207,203,193,131,104,84,81,77,76,51,24,22,12,2]\n",
    "all_instances = list(range(_TOT_NUM_INSTANCES))\n",
    "# all_instances = list(np.loadtxt('/mnt/new_disk2/liury/log/SRNs/goodList.txt'))\n",
    "for i in range(100):\n",
    "\n",
    "    lat_idx = [all_instances[i], random.choice(all_instances)]\n",
    "    src_idx = lat_idx[0]\n",
    "    trgt_idx = (lat_idx[0]+13)%_TOT_NUM_INSTANCES\n",
    "    \n",
    "    src_latent = model.get_embedding({'instance_idx': torch.LongTensor([src_idx]).squeeze().cuda()}).unsqueeze(0)\n",
    "    trgt_latent = model.get_embedding({'instance_idx': torch.LongTensor([trgt_idx]).squeeze().cuda()}).unsqueeze(0)\n",
    "    \n",
    "    vis_outputs = []\n",
    "    output_dir = os.path.join(\n",
    "        _LOG_ROOT, 's%04d_t%04d'%(src_idx, trgt_idx))\n",
    "    os.makedirs(os.path.join(output_dir), exist_ok=True)\n",
    "    \n",
    "    cam_K = '../checkpoints/intrinsics.txt'\n",
    "    cam_int = data_util.parse_intrinsics(cam_K, trgt_sidelength=128)\n",
    "    focal = cam_int[0, 0]\n",
    "    cx, cy = cam_int[:2, 2]\n",
    "\n",
    "    # ROTATE\n",
    "    n_poses=30\n",
    "    R = np.linalg.norm(cam_center) + radii[0]\n",
    "    theta = []\n",
    "    theta_range = [0, -0.38, 0.38, 0]\n",
    "    for i in range(len(theta_range)-1):\n",
    "        theta.append( np.linspace(theta_range[i],theta_range[i+1], num=n_poses//4))\n",
    "    theta = np.concatenate(theta)\n",
    "    x = R*np.sin(theta)\n",
    "    y = np.zeros_like(x)\n",
    "    z = R*np.cos(theta)\n",
    "    cam_T = np.stack([x,y,z],axis=1) + lookat.reshape((1,3))\n",
    "\n",
    "    for i in range(len(theta)):\n",
    "        cam_pose = _campos2matrix(cam_T[i], lookat)\n",
    "        out_img, out_seg = render_scene(model, cam_pose, src_latent, focal, _OUT_SIZE)  \n",
    "        vis_outputs.append(out_img)\n",
    "        output_fp = os.path.join(output_dir, '%04d.png'%(len(vis_outputs)))\n",
    "        util.write_img(out_seg, output_fp)\n",
    "    \n",
    "    # SPPIRAL PATH\n",
    "#     cam_poses = create_spiral_poses(radii,focus_depth)\n",
    "    t = np.linspace(0, 4*np.pi, n_poses+1)[:-1]\n",
    "    for k in range(n_poses):\n",
    "        cam_T = np.array([np.cos(t[k]), -np.sin(t[k]), -np.sin(0.5*t[k])]) * radii\n",
    "        cam_T = cam_T[[1,2,0]] + cam_center\n",
    "        cam_pose = _campos2matrix(cam_T, lookat)\n",
    "        out_img, out_seg = render_scene(model, cam_pose, src_latent, focal, _OUT_SIZE)  \n",
    "        vis_outputs.append(out_img)\n",
    "        output_fp = os.path.join(output_dir, '%04d.png'%(len(vis_outputs)))\n",
    "        util.write_img(out_seg, output_fp)\n",
    "\n",
    "    cdf_scale = 1.0/(1.0-norm.cdf(-_INTERP_STEPS//2,0,6)*2)\n",
    "    for idx in range(-_INTERP_STEPS//2,_INTERP_STEPS//2+1):\n",
    "        _w = norm.cdf(idx,0,6)*cdf_scale\n",
    "        latent = (1.0-_w)*src_latent + _w*trgt_latent\n",
    "        out_img, out_seg = render_scene(model, cam_pose, latent, focal, _OUT_SIZE) \n",
    "        vis_outputs.append(out_img)\n",
    "        output_fp = os.path.join(output_dir, '%04d.png'%(len(vis_outputs)))\n",
    "        util.write_img(out_seg, output_fp)\n",
    "        \n",
    "    for k in range(n_poses):\n",
    "        cam_T = np.array([np.cos(t[k]), -np.sin(t[k]), -np.sin(0.5*t[k])]) * radii\n",
    "        cam_T = cam_T[[1,2,0]] + cam_center\n",
    "        cam_pose = _campos2matrix(cam_T, lookat)\n",
    "        out_img, out_seg = render_scene(model, cam_pose, trgt_latent, focal, _OUT_SIZE)  \n",
    "        vis_outputs.append(out_img)\n",
    "        output_fp = os.path.join(output_dir, '%04d.png'%(len(vis_outputs))) \n",
    "        util.write_img(out_seg, output_fp)\n",
    "\n",
    "    for idx in range(-_INTERP_STEPS//2,_INTERP_STEPS//2+1):\n",
    "        _w = norm.cdf(idx,0,6)*cdf_scale\n",
    "        latent = (1.0-_w)*trgt_latent + _w*src_latent\n",
    "        out_img, out_seg = render_scene(model, cam_pose, latent, focal, _OUT_SIZE) \n",
    "        vis_outputs.append(out_img)\n",
    "        output_fp = os.path.join(output_dir, '%04d.png'%(len(vis_outputs)))\n",
    "        util.write_img(out_seg, output_fp)\n",
    "        \n",
    "    vis_fp = os.path.join(_LOG_ROOT, 'vis', os.path.basename(output_dir)+'.gif')\n",
    "    print('[DONE] save vis to : ', vis_fp)\n",
    "    imageio.mimsave(vis_fp, vis_outputs, fps=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.       , -0.       ,  0.       ,  0.       ],\n",
       "       [-0.       , -1.       ,  0.       ,  0.12     ],\n",
       "       [ 0.       , -0.       , -1.       ,  1.0773945],\n",
       "       [ 0.       ,  0.       ,  0.       ,  1.       ]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam_pose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DEFAULT_CAM_INT = '../checkpoints/intrinsics.txt'\n",
    "_LOG_ROOT = os.path.join(os.getenv(\"HOME\"), 'liury/log/SRNs')\n",
    "\n",
    "cam_int = data_util.parse_intrinsics(_DEFAULT_CAM_INT, trgt_sidelength=128)\n",
    "focal = cam_int[0, 0]\n",
    "cam_center = np.array([0,0.11,0.1])\n",
    "\n",
    "\n",
    "for i in range(150):\n",
    "    lat_idx = torch.randint(0, _TOT_NUM_INSTANCES, (2,)).squeeze().cuda()\n",
    "    src_latent = model.get_embedding({'instance_idx': lat_idx[0]}).unsqueeze(0)\n",
    "    trgt_latent = model.get_embedding({'instance_idx': lat_idx[1]}).unsqueeze(0)\n",
    "    \n",
    "    src_idx = lat_idx[0].cpu().numpy()\n",
    "    trgt_idx = lat_idx[1].cpu().numpy()\n",
    "        \n",
    "    output_dir = os.path.join(\n",
    "        _LOG_ROOT, 'interp_latent', 's%04d_t%04d'%(src_idx, trgt_idx))\n",
    "    os.makedirs(os.path.join(output_dir, 'seg'), exist_ok=True)\n",
    "    \n",
    "    print('[%02d] %04d -> %04d: %s'%(i, src_idx, trgt_idx, output_dir))\n",
    "    \n",
    "    R = np.random.rand()*0.5 + 0.7\n",
    "    theta = np.random.rand()*0.4 + (np.pi/2-0.2)\n",
    "    phi = np.random.rand()*1.2 + (np.pi/2-0.6)\n",
    "    \n",
    "    x = R * np.sin(theta) * np.cos(phi)\n",
    "    y = R * np.sin(theta) * np.sin(phi)\n",
    "    z = R * np.cos(theta)\n",
    "    \n",
    "    cam_pose = _campos2matrix(np.array([x, z, y])+cam_center, cam_center)\n",
    "    \n",
    "    steps = 30\n",
    "    \n",
    "    img_outputs = []    \n",
    "    \n",
    "    for idx in range(steps):\n",
    "        _w = float(idx) / (steps - 1)\n",
    "        latent = (1.0-_w)*src_latent + _w*trgt_latent\n",
    "        \n",
    "        out_img, out_seg = render_scene(model, cam_pose, latent, focal, _OUT_SIZE)\n",
    "        \n",
    "        img_outputs.append(out_img)\n",
    "        output_fp = os.path.join(output_dir, '%04d.png'%(idx))\n",
    "        util.write_img(out_seg, output_fp)\n",
    "        \n",
    "    imageio.mimsave(os.path.join(output_dir, 'output.gif'), img_outputs, fps=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot latents:  80\n",
      "torch.Size([262144, 20]) torch.Size([262144, 20])\n"
     ]
    }
   ],
   "source": [
    "# load latent\n",
    "from glob import glob\n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "\n",
    "from dataset import data_util\n",
    "from dataset.data_util import load_seg_map\n",
    "\n",
    "_ORI_DATA_ROOT = '/mnt/new_disk2/liury/data/facial-data/CelebAMask-HQ/segmap_20'\n",
    "_LOG_ROOT = os.path.join(os.getenv(\"HOME\"), 'liury/log/SRNs')\n",
    "_LATENT_ROOT = os.path.join(_LOG_ROOT, '060115face_celebA/latent_codes')\n",
    "\n",
    "all_latents = glob(os.path.join(_LATENT_ROOT, '*.npy'))\n",
    "\n",
    "print('tot latents: ', len(all_latents))\n",
    "\n",
    "_DEFAULT_CAM_INT = '../checkpoints/intrinsics.txt'\n",
    "cam_int = data_util.parse_intrinsics(_DEFAULT_CAM_INT, trgt_sidelength=128)\n",
    "focal = cam_int[0, 0]\n",
    "cam_center = np.array([0,0.11,0.0])\n",
    "\n",
    "for i in range(27, len(all_latents)):\n",
    "    out_imgs = []    \n",
    "    \n",
    "    item_id = int(os.path.basename(all_latents[i]).split('.')[0].split('_')[1])\n",
    "    latent = np.load(all_latents[i])\n",
    "    \n",
    "    # load ori\n",
    "    ori_seg = torch.from_numpy(load_seg_map(os.path.join(_ORI_DATA_ROOT, '%d.png'%(item_id)), _OUT_SIZE))\n",
    "    ori_seg_img = util.lin2img(ori_seg.unsqueeze(0), color_map=_CMAP).cpu().numpy()\n",
    "    ori_seg_img = (ori_seg_img.squeeze().transpose(1, 2, 0)) * 255.0\n",
    "    ori_seg_img = ori_seg_img.round().clip(0, 255).astype(np.uint8)\n",
    "    out_imgs.append(torch.from_numpy(ori_seg_img).permute(2, 0, 1).unsqueeze(0))\n",
    "        \n",
    "    # predict\n",
    "    cam_T = latent[:3]\n",
    "    latent = torch.from_numpy(latent[3:]).unsqueeze(0).cuda()\n",
    "    R = np.linalg.norm(cam_T)\n",
    "\n",
    "    cam_pose = _campos2matrix(cam_T+cam_center, cam_center)\n",
    "    out_img, out_seg = render_scene(model, cam_pose, latent, focal, _OUT_SIZE)\n",
    "    out_imgs.append(torch.from_numpy(out_img).permute(2, 0, 1).unsqueeze(0))\n",
    "    \n",
    "    # calc mIoU\n",
    "    ori_seg = F.one_hot(ori_seg.squeeze().long(), 20)\n",
    "    out_seg = F.one_hot(torch.from_numpy(out_seg).flatten().long(), 20)\n",
    "    \n",
    "    print(ori_seg.shape, out_seg.shape)\n",
    "    break\n",
    "    \n",
    "    mIoU = torch.mean(torch.div(\n",
    "        torch.sum(ori_seg&out_seg, dim=0).float()+1e-8,\n",
    "        torch.sum(ori_seg|out_seg, dim=0).float()+1e-8))\n",
    "                \n",
    "    for step in range(6):\n",
    "    \n",
    "        theta = np.random.rand()*0.4 + (np.pi/2-0.2)\n",
    "        phi = np.random.rand()*1.2 + (np.pi/2-0.6)\n",
    "\n",
    "        x = R * np.sin(theta) * np.cos(phi)\n",
    "        y = R * np.sin(theta) * np.sin(phi)\n",
    "        z = R * np.cos(theta)\n",
    "\n",
    "        cam_pose = _campos2matrix(np.array([x, z, y])+cam_center, cam_center)\n",
    "        out_img, out_seg = render_scene(model, cam_pose, latent, focal, _OUT_SIZE)\n",
    "                \n",
    "        out_imgs.append(torch.from_numpy(out_img).permute(2, 0, 1).unsqueeze(0))\n",
    "    \n",
    "        \n",
    "    out_imgs = make_grid(torch.cat(out_imgs), nrow=8, padding=1).permute((1, 2, 0)).cpu().numpy()\n",
    "    \n",
    "    ############## vis ##################    \n",
    "    fig = plt.figure(figsize=(45, 5))\n",
    "    plt.imshow(out_imgs)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    ####################################\n",
    "    \n",
    "    output_dir = os.path.join(_LOG_ROOT, 'change_view_celebA', '%06d'%(item_id))\n",
    "    os.makedirs(os.path.join(output_dir, 'seg'), exist_ok=True)    \n",
    "    render_custo_path(0, latent, cam_center=cam_center, output_dir=output_dir)\n",
    "    \n",
    "    print('[%06d] %d : R = %f, mIoU = %f, output_dir = %s'%(i, item_id, R, mIoU, output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = torch.cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    lat_idx = torch.randint(0, _TOT_NUM_INSTANCES, (2,)).squeeze().cuda()\n",
    "    src_latent = model.get_embedding({'instance_idx': lat_idx[0]}).unsqueeze(0)\n",
    "    trgt_latent = model.get_embedding({'instance_idx': lat_idx[1]}).unsqueeze(0)\n",
    "    \n",
    "    src_idx = lat_idx[0].cpu().numpy()\n",
    "    trgt_idx = lat_idx[1].cpu().numpy()\n",
    "    \n",
    "    print('%02d: %04d -> %04d'%(i, src_idx, trgt_idx))\n",
    "    \n",
    "    output_dir = os.path.join(\n",
    "        _LOG_ROOT, 'out_mode_3', 's%04d_t%04d'%(src_idx, trgt_idx))\n",
    "    os.makedirs(os.path.join(output_dir, 'seg'), exist_ok=True)\n",
    "    \n",
    "    print(output_dir)\n",
    "    \n",
    "    render_custo_path(0, src_latent, trgt_latent, output_dir=output_dir)\n",
    "\n",
    "    cam_pose = _campos2matrix(np.array([0., 0., 0.8])+cam_center, cam_center)\n",
    "    out_img, out_seg = render_scene(model, cam_pose, _get_latent(i), focal, _OUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.face_dataset import FaceRandomPoseDataset\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "\n",
    "_OUTPUT_DIR = os.path.join(_LOG_ROOT, 'output_iter_020000')\n",
    "_MODE = 'sphere'\n",
    "_R = 1.5\n",
    "\n",
    "\n",
    "_NUM_INSTANCES=10\n",
    "_NUM_OBSERVATIONS=25\n",
    "\n",
    "output_dir = os.path.join(_OUTPUT_DIR, _MODE+'_128')\n",
    "sample_instances = list(np.random.choice(range(_TOT_NUM_INSTANCES), _NUM_INSTANCES, replace=False))\n",
    "\n",
    "# writer = SummaryWriter(output_dir)\n",
    "\n",
    "dataset = FaceRandomPoseDataset(\n",
    "    intrinsics=_CAM_INT,\n",
    "    cam_center=_CAM_CENTER,\n",
    "    num_instances=sample_instances, \n",
    "    num_observations=_NUM_OBSERVATIONS, \n",
    "    sample_radius=_R, mode=_MODE)\n",
    "\n",
    "dataloader = DataLoader(dataset,\n",
    "                     collate_fn=dataset.collate_fn,\n",
    "                     batch_size=1,\n",
    "                     shuffle=False,\n",
    "                     drop_last=False)\n",
    "\n",
    "print('Beginning evaluation...')\n",
    "\n",
    "images = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for idx, model_input in enumerate(dataloader):\n",
    "        model_input, ground_truth = model_input\n",
    "        \n",
    "        pose = model_input['pose']\n",
    "        \n",
    "#         print(pose)\n",
    "        \n",
    "        intrinsics = model_input['intrinsics']\n",
    "        uv = model_input['uv']\n",
    "        z = model.get_embedding(model_input)\n",
    "        \n",
    "        model_outputs = model(pose, z, intrinsics, uv)\n",
    "        predictions, depth_maps = model_outputs\n",
    "                \n",
    "        batch_size, tensor_len, channels = predictions.shape\n",
    "        img_sidelen = np.sqrt(tensor_len).astype(int)\n",
    "                \n",
    "        pred = torch.argmax(predictions, dim=2, keepdim=True)\n",
    "        output_img = util.lin2img(pred, color_map=).cpu().numpy()\n",
    "        output_pred = pred.view(batch_size, img_sidelen, img_sidelen, 1).cpu().numpy()\n",
    "\n",
    "        for i in range(output_img.shape[0]):\n",
    "            instance_idx = int(model_input['instance_idx'][i].squeeze().detach().cpu().numpy().astype(np.int64))\n",
    "            observation_idx = model_input['observation_idx'][i]\n",
    "            \n",
    "            instance_dir = os.path.join(output_dir, \"%03d\" % instance_idx)\n",
    "            os.makedirs(instance_dir, exist_ok=True)\n",
    "            \n",
    "            img = output_img[i, :, :, :].squeeze().transpose(1, 2, 0)\n",
    "            img *= 255\n",
    "            img = img.round().clip(0, 255).astype(np.uint8)\n",
    "#             img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "\n",
    "            if not instance_idx in images:\n",
    "                images[instance_idx] = [None] * _NUM_OBSERVATIONS\n",
    "\n",
    "            images[instance_idx][observation_idx] = img\n",
    "#             output_fp = os.path.join(instance_dir, '%02d.png'%(observation_idx))\n",
    "#             util.write_img(img, output_fp)\n",
    "            \n",
    "            output_fp = os.path.join(instance_dir, '%02d_seg.png'%(observation_idx))\n",
    "            seg_img = output_pred[i, :, :].squeeze().astype(np.uint8)\n",
    "            \n",
    "            util.write_img(seg_img, output_fp)\n",
    "            \n",
    "#             dpt_img = (output_dpt[i, :, :, :].squeeze() * 255.0).astype(np.uint8)\n",
    "#             dpt_img = cv2.applyColorMap(dpt_img, cv2.COLORMAP_JET)\n",
    "#             output_fp = os.path.join(instance_dir, '%02d_depth.png'%(observation_idx))\n",
    "# #             print('Save output for instance %03d - %02d: %s'%(instance_idx, observation_idx, output_fp))\n",
    "#             util.write_img(dpt_img, output_fp)\n",
    "            print('Save output for instance %04d - %02d: %s'%(instance_idx, observation_idx, output_fp))\n",
    "        \n",
    "            if observation_idx == _NUM_OBSERVATIONS - 1:\n",
    "                imageio.mimsave(os.path.join(instance_dir, 'output.gif'), images[instance_idx], fps=5.0)\n",
    "                print('=== [DONE] saving output.gif.')\n",
    "            \n",
    "#         print('[DONE] Save output for instance %03d.'%(instance_idx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# move seg_face_8000\n",
    "import os\n",
    "from glob import glob\n",
    "from shutil import copyfile\n",
    "\n",
    "data_root = '/data/anpei/facial-data/seg_face_8000/images'\n",
    "img_fps = glob(os.path.join(data_root, '*_seg_*.png'))\n",
    "img_ids = list(set([os.path.basename(x)[:5] for x in img_fps]))\n",
    "\n",
    "for img_id in img_ids:\n",
    "    output_dir = os.path.join(data_root, img_id)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    copyfile(\n",
    "        os.path.join(data_root, '..', 'cam2world.npy'), \n",
    "        os.path.join(output_dir, 'cam2world.npy'))\n",
    "    \n",
    "    img_fps = glob(os.path.join(data_root, '%s_seg_*.png'%(img_id)))\n",
    "    print(output_dir, len(imgs))\n",
    "    for img_fp in sorted(img_fps):\n",
    "        out_name = os.path.basename(img_fp).split('_')\n",
    "        out_sub_dir = out_name[0]\n",
    "        out_name = '_'.join([out_name[1], '%02d'%(int(out_name[-2])*5+int(out_name[-1][:2]))]) + '.png'\n",
    "        out_name = os.path.join(os.path.dirname(img_fp), out_sub_dir, out_name)\n",
    "#         print('\\t> ', img_fp, out_name)\n",
    "        os.rename(img_fp, out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "data_dir = '/data/anpei/facial-data/seg_face_2000/10996'\n",
    "\n",
    "cam_param = np.load(os.path.join(data_dir, 'cameras.npy'), allow_pickle=True).item() \n",
    "print(np.asarray(cam_param['zRange']).shape, np.asarray(cam_param['extrinsics']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as torch_models\n",
    "import torch\n",
    "\n",
    "resnet_model = torch_models.resnet18(pretrained=False)\n",
    "# resnet_model.layer4 = torch.nn.Identity()\n",
    "# resnet_model.fc = torch.nn.Identity()\n",
    "resnet_model.conv1 = torch.nn.Conv2d(19, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "print(resnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = resnet_model(torch.Tensor(8, 19, 128, 128))\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "aa = torch.randint(0, 19, size=(1, 3, 3))\n",
    "print(aa.shape)\n",
    "\n",
    "bb = F.one_hot(aa)\n",
    "print(bb.shape, bb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = torch.empty(256)\n",
    "print(aa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "instance_dir = '/data/anpei/facial-data/seg_body/94'\n",
    "seg_imgs = img = cv2.imread(os.path.join(instance_dir, 'semantic_mask', 'image.cam05_000094.png'), cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "print(np.unique(seg_imgs))\n",
    "\n",
    "\n",
    "fs = cv2.FileStorage(\"/data/anpei/facial-data/seg_body/Calib_T_samba/cam05_000000/intrinsic.xml\", cv2.FILE_STORAGE_READ)\n",
    "intrinsics = fs.getNode(\"M\").mat()\n",
    "\n",
    "print(intrinsics)\n",
    "\n",
    "H, W = seg_imgs.shape\n",
    "cx, cy = intrinsics[:2, 2]\n",
    "\n",
    "x_coord, y_coord = np.where(seg_imgs != 0)\n",
    "bbox = np.asarray([np.min(x_coord), np.min(y_coord), np.max(x_coord), np.max(y_coord)])\n",
    "print(bbox)\n",
    "\n",
    "sidelength = max(bbox[2] - bbox[0], bbox[3] - bbox[1])\n",
    "center = np.asarray([bbox[2] + bbox[0], bbox[3] + bbox[1]]) / 2.0\n",
    "bbox = np.ceil(np.asarray([\n",
    "    center[0]-sidelength/2.0, \n",
    "    center[1]-sidelength/2.0, \n",
    "    center[0]+sidelength/2.0,\n",
    "    center[1]+sidelength/2.0])).astype(np.int64)\n",
    "\n",
    "cx, cy = intrinsics[:2, 2] - bbox[:2]\n",
    "\n",
    "print(bbox, bbox.dtype)\n",
    "seg_imgs = seg_imgs[bbox[0]:bbox[2], bbox[1]:bbox[3]]\n",
    "print(seg_imgs.shape)\n",
    "\n",
    "\n",
    "print(cx, cy, bbox)\n",
    "intrinsics[:2, 2] = [cx, cy]\n",
    "print(intrinsics)\n",
    "\n",
    "\n",
    "print(H, W, cx, cy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.body_dataset import BodyPartDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import util\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "dataset = BodyPartDataset(\n",
    "        data_type='seg',\n",
    "        img_sidelength=128, \n",
    "        sample_observations=list(range(5)),\n",
    "        sample_instances=list(range(2)))\n",
    "\n",
    "dataloader = DataLoader(dataset,\n",
    "                     collate_fn=dataset.collate_fn,\n",
    "                     batch_size=1,\n",
    "                     shuffle=False,\n",
    "                     drop_last=False)\n",
    "\n",
    "print(len(dataloader))\n",
    "\n",
    "for idx, model_input in enumerate(dataloader):\n",
    "    model_input, ground_truth = model_input\n",
    "        \n",
    "    seg_img = model_input['rgb']\n",
    "    pose = model_input['pose']\n",
    "    intrinsics = model_input['intrinsics']\n",
    "    uv = model_input['uv']    \n",
    "    \n",
    "    print('> ', np.unique(seg_img), seg_img.shape)\n",
    "    print('> pose = ')\n",
    "    print(pose)\n",
    "    print('> cam_int = ')\n",
    "    print(intrinsics)\n",
    "    print(seg_img.shape, np.unique(seg_img))\n",
    "    \n",
    "    output_img = util.lin2img(seg_img, color_map=dataset.color_map).cpu().numpy()\n",
    "    \n",
    "    plt.imshow(output_img.squeeze().transpose(1, 2, 0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "model = models.resnet18(pretrained=False)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "H, W = 512, 512\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(   self, \n",
    "                    in_channels, \n",
    "                    out_channels, \n",
    "                    kernel_size=3, \n",
    "                    stride=1, \n",
    "                    num_groups=2):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, stride//2),\n",
    "            nn.GroupNorm(num_groups, out_channels),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        self.conv1_1 = ConvBlock(in_channels, 8, 3, 1)\n",
    "        self.conv1_2 = ConvBlock(8, 16, 3, 2)\n",
    "        self.conv2_1 = ConvBlock(16, 16, 3, 1)\n",
    "        self.conv2_2 = ConvBlock(16, 32, 3, 2)\n",
    "        self.conv3_1 = ConvBlock(32, 32, 3, 1)\n",
    "        self.deconv4_1 = nn.ConvTranspose2d(32, 16, 3, 2)\n",
    "\n",
    "        self.conv4_2 = ConvBlock(32, 16, 3, 1)\n",
    "        self.deconv5_1 = nn.ConvTranspose2d(16, 8, 3, 2)\n",
    "\n",
    "        self.conv5_2 = ConvBlock(16, 8, 3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.conv1_1(x)\n",
    "        print(x0.shape)\n",
    "        x = self.conv1_2(x0)\n",
    "        x1 = self.conv2_1(x)\n",
    "        x = self.conv2_2(x)\n",
    "        out0 = self.conv3_1(x)\n",
    "        x = self.deconv4_1(out0)\n",
    "\n",
    "        x = torch.cat([x1, x], dim=1)\n",
    "        out1 = self.conv4_2(x)\n",
    "        x = self.deconv5_1(out1)\n",
    "        x = torch.cat([x0, x], dim=1)\n",
    "\n",
    "        out2 = self.conv5_2(x)\n",
    "        return out0, out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_in = torch.rand(8, 3, H, W)\n",
    "print(fake_in.shape)\n",
    "\n",
    "feat_worker = FeatureExtractor()\n",
    "\n",
    "output = feat_worker(fake_in)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from image_utils import Random_Transforms\n",
    "\n",
    "basedir='/mnt/wmy/sport_1_mask'\n",
    "cam_id=0\n",
    "\n",
    "poses = np.loadtxt(os.path.join(basedir,'CamPose.inf'))\n",
    "Ks = read_intrinsics(os.path.join(basedir,'Intrinsic.inf'))\n",
    "\n",
    "img_fp = os.path.join(basedir, 'img', '%d/img_%04d.jpg' % (_FRAME_ID, cam_id))\n",
    "img = imageio.imread(img_fp)\n",
    "mask_fp = os.path.join(basedir, 'img', 'mask', '%d/img_%04d.jpg' % (_FRAME_ID, cam_id))\n",
    "mask = imageio.imread(mask_fp)\n",
    "\n",
    "img, K, img_mask, ROI = transform(img, Ks[cam_id], mask)\n",
    "\n",
    "print(img.shape)\n",
    "\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "img_fp = os.path.join(basedir, 'img', '%d/img_%04d.jpg' % (0, 0))\n",
    "img = imageio.imread(img_fp)\n",
    "\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Ks[0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(f, x='Hi there!');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
